{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Download the dataset from PhysioNet: https://physionet.org/content/challenge-2017/1.0.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "random.seed(42)\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "physionet_dir = '/physionet.org/files/challenge-2017/1.0.0/training/'\n",
    "raw_data_dir = home_dir + physionet_dir\n",
    "\n",
    "splited_data_dir = './data/'\n",
    "os.makedirs(splited_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Loop through the subfolders to find all the .mat and .hea files\n",
    "\n",
    "Four RECORDS files are provided in the dataset to record all the index of the .mat and .hea files for each class. We can use the RECORDS to find all the .mat and .hea files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal records: 5076\n",
      "af records: 758\n",
      "noisy records: 279\n",
      "other records: 2415\n"
     ]
    }
   ],
   "source": [
    "# read RECORDS-normal file\n",
    "records_list = ['normal', 'af', 'noisy', 'other']\n",
    "records_dic = {}\n",
    "\n",
    "for item in records_list:\n",
    "    with open (raw_data_dir + f'RECORDS-{item}') as f:\n",
    "        records = f.readlines()\n",
    "        records = [x.strip() for x in records]\n",
    "\n",
    "    # store the records into a dic\n",
    "    records_dic[item] = records\n",
    "    print(f'{item} records: {len(records)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the dataset into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4060\n",
      "Testing set: 4468\n"
     ]
    }
   ],
   "source": [
    "# Training set: randomly select 80% of the data in records-normal\n",
    "# Testing set: randomly select 20% of the data in records-normal, 100% of the data in records-af, records-noisy, records-other\n",
    "# also generate a Reference csv for training and testing set to store labels\n",
    "\n",
    "training_set = []\n",
    "testing_set = []\n",
    "training_label = []\n",
    "testing_label = []\n",
    "\n",
    "for item in records_list:\n",
    "    if item == 'normal':\n",
    "        records = records_dic[item]\n",
    "        random.shuffle(records)\n",
    "        training_set += records[:int(len(records)*0.8)]\n",
    "        training_label += [item]*int(len(records)*0.8)\n",
    "        testing_set += records[int(len(records)*0.8):]\n",
    "        testing_label += [item]*(len(records) - int(len(records)*0.8))\n",
    "    else:\n",
    "        records = records_dic[item]\n",
    "        testing_set += records\n",
    "        testing_label += [item]*len(records)\n",
    "\n",
    "\n",
    "print(f\"Training set: {len(training_set)}\")\n",
    "print(f\"Testing set: {len(testing_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the existing files\n",
    "shutil.rmtree(splited_data_dir, ignore_errors=True)\n",
    "\n",
    "# copy .mat and .hea files to the corresponding folder\n",
    "os.makedirs(splited_data_dir + 'training/', exist_ok=True)\n",
    "for item in training_set:\n",
    "    shutil.copy(raw_data_dir +\n",
    "                item + '.mat', splited_data_dir + 'training/')\n",
    "    shutil.copy(raw_data_dir +\n",
    "                item + '.hea', splited_data_dir + 'training/')\n",
    "os.makedirs(splited_data_dir + 'testing/', exist_ok=True)\n",
    "for item in testing_set:\n",
    "    shutil.copy(raw_data_dir +\n",
    "                item + '.mat', splited_data_dir + 'testing/')\n",
    "    shutil.copy(raw_data_dir +\n",
    "                item + '.hea', splited_data_dir + 'testing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# remove the first 4 characters of the file name\n",
    "# save the label into a csv file\n",
    "\n",
    "training_df = pd.DataFrame({'filename': [x[4:] for x in training_set], 'label': training_label})\n",
    "testing_df = pd.DataFrame({'filename': [x[4:] for x in testing_set], 'label': testing_label})\n",
    "training_df.to_csv(splited_data_dir + 'training.csv', index=False)\n",
    "testing_df.to_csv(splited_data_dir + 'testing.csv', index=False)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zhipenghe/GitHub/PhysioNet-CinC-2017/data.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the folder\n",
    "shutil.make_archive('data', 'zip', './data/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cinc2017",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
